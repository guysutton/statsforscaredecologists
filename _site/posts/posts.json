[
  {
    "path": "posts/2022-03-24-building-your-first-bayesian-glm-using-turingjl-in-julia/",
    "title": "Fitting your first Bayesian GLM in Julia",
    "description": "Use Turing.jl to fit Bayesian regressions",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology"
      }
    ],
    "date": "2022-03-24",
    "categories": [
      "Julia",
      "GLM",
      "Bayesian",
      "Turing",
      "Turing.jl",
      "Model fitting"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nLoad required packages\r\nLoad data\r\nFitting a model\r\nStep 1: Specify the model\r\nStep 2: Sampling the posterior\r\nStep 3: Evaluate the model fit\r\nStep 4: Model inference\r\n\r\nCompare with frequentist approach\r\nConclusion\r\n\r\n\r\n\r\n\r\nBackground\r\nIn earlier posts, I have demonstrated how to fit and interpret the output of simple Gaussian GLM’s using both R and Julia. However, the field of ecology (and other fields, e.g. economics, health sciences) all seem to be shifting their focus and attention to Bayesian data analysis (BDA), over the traditional frequentest approaches (e.g. null-hypothesis significance testing). I encourage anyone who is interested in the long-standing debate of using frequentist vs bayesian methods to consult people more knowledgeable than me (e.g. here, here, and here).\r\nVery simply, Bayesian methods differ from traditional frequentist methods in two primary ways:\r\nPriors: Frequentist approaches assume that all of the information we can derive must come from the underlying data (e.g. model parameters). Bayesian methods allow us to incorporate other information (e.g. literature), or an uninformed guess, about what the model parameters could be, called a prior. Where we don’t have any information, we can fall back into non-informative priors, such as the normal (Gaussian) distribution.\r\nPosterior: A Bayesian approach allows us to calculate a distribution of possible models parameters based on the actual data and the prior information we have. Because we are calculating a distribution of model parameters, we can also quantify uncertainty around these estimates. This allows us to make probabilistic statements such as: given the model we fit, the data and the prior information we have about the topic, there is an Z% chance that the effect of covariate X increases Y by so much… This is a bit wordy, but it will hopefully be more clear with an example.\r\nMy potentially oversimplified summary of Bayesian vs frequentist approaches is that Bayesian methods aim to generate a probability distribution of possible model paramters, while frequentist methods are geared towards trying to estimate a single best estimate of the model parameters.\r\nIn today’s blogpost, I want to demonstrate the basics of fitting, evaluating and interpreting a simple Bayesian linear regression in Julia. I am by no means an expert in Bayesian analysis, so if any experts are reading this and have any pointers, please let me know. This post will be purposely kept as simple as possible to serve as a light and breezy introduction to an otherwise challenging and complex statistical modelling toolbox.\r\nPlease note, there is an apparent dependency issue with installing TuringGLM.jl in R that doesn’t seem to have an easy fix (at least not for me), which I use to write and host my blogposts. As such, I will be showing some screenshots of the required output when I run the analysis in Julia.\r\nLoad required packages\r\nTo fit a Bayesian regression model in Julia, we are going to take advantage of the amazing Turing.jl package in Julia. Turing.jl is a general-purpose probabilistic programming package that allows Julia users to fit Bayesian models using standard Julia syntax. For more details, please consult the Turing.jl homepage. In fact, we are actually going to use another package, TuringGLM.jl to specify our models using simple coding syntax, and it will return an instantiated Turing model without us having to lift a finger, so to speak. The syntax of Turing.jl is very similar to using brms in R (insert ref), which makes the transition between languages easy.\r\n\r\n# Load the package manager\r\nusing Pkg\r\n\r\n# # Install required packages (once-off)\r\n# Pkg.add(\"GLM\")             # General linear models (GLM)\r\n# Pkg.add(\"StatsModels\")     # Perform likelihood-ratio test\r\n# Pkg.add(\"DataFrames\")      # Manipulating data structures (Julia's version of 'dplyr')\r\n# Pkg.add(\"StatsBase\")       # Basic statistical functions (e.g. mean, stddev)\r\n# Pkg.add(\"Statistics\")      # More basic statistical functions \r\n# Pkg.add(\"Distributions\")   # Fit basic statistical distributions \r\n# Pkg.add(\"Plots\")           # Plot figures \r\n# Pkg.add(\"StatsPlots\")      # Grouped box-plots recipe\r\n# Pkg.add(\"Turing\")          # Dependency for probabilistic probability calculations \r\n# Pkg.add(\"TuringGLM\")       # Bayesian GLM's\r\n\r\n# # Load required packages (every new session)\r\nusing GLM                    # General linear models (GLM)\r\nusing StatsModels            # Perform likelihood-ratio test\r\nusing DataFrames             # Manipulating data structures (Julia's version of 'dplyr')\r\nusing StatsBase              # Basic statistical functions (e.g. mean, stddev)\r\nusing Statistics             # More basic statistical functions \r\nusing Distributions          # Fit basic statistical distributions \r\nusing Plots                  # Plot figures \r\nusing StatsPlots             # Grouped box-plots recipe\r\nusing Turing                 # Dependency for probabilistic probability calculations \r\n# using TuringGLM              # Fit Bayesian GLM's \r\nusing RCall                  # Port the 'Salamanders' dataset from R's 'glmmTMB' package \r\n\r\nLoad data\r\nLet’s read in the salamanders dataset that is built-in to the glmmTMB package in R (insert ref). The salamanders dataset consists of counts (abundances) of a number of different salamander species from 23 different sites. There are a number of site (e.g. the amount of cover objects in the stream cover) and sampling covariates in the dataset (e.g. the day of the year each sampling event occurred DOY). Each site was sampled on 4 separate occasions, meaning that there is a hierarchical structure in the data, however, let’s not worry about that for today (we will come back to this in a later blogpost).\r\n\r\n# Load in the dataset \r\nsalamanders = RCall.rcopy(R\"glmmTMB::Salamanders\")\r\n644×9 DataFrame\r\n Row │ site   mined  cover       sample  DOP         Wtemp       DOY        sp ⋯\r\n     │ Cat…   Cat…   Float64     Int64   Float64     Float64     Float64    Ca ⋯\r\n─────┼──────────────────────────────────────────────────────────────────────────\r\n   1 │ VF-1   yes    -1.44232         1  -0.595683   -1.22938    -1.497     GP ⋯\r\n   2 │ VF-2   yes     0.29841         1  -0.595683    0.0847653  -1.497     GP\r\n   3 │ VF-3   yes     0.397881        1  -1.19137     1.01418    -1.29447   GP\r\n   4 │ R-1    no     -0.447616        1   0.0        -3.02336    -2.71222   GP\r\n   5 │ R-2    no      0.596821        1   0.595683   -0.144345   -0.68686   GP ⋯\r\n   6 │ R-3    no      1.34285         1   0.595683   -0.0146601  -0.68686   GP\r\n   7 │ R-4    no     -0.944966        1  -0.595683   -0.446944   -0.97041   GP\r\n   8 │ R-5    no      0.497351        1  -0.595683   -0.602567   -0.97041   GP\r\n  ⋮  │   ⋮      ⋮        ⋮         ⋮         ⋮           ⋮           ⋮       ⋮ ⋱\r\n 638 │ VF-6   yes    -0.397881        4  -0.0915999   0.510846    1.46002   DF ⋯\r\n 639 │ R-12   no      0.795761        4  -0.0915999  -0.188417    1.25748   DF\r\n 640 │ VF-7   yes    -0.696291        4  -0.0915999   2.05658     1.41951   DF\r\n 641 │ VF-9   yes     0.447616        4  -0.0915999   0.768469    1.46002   DF\r\n 642 │ VF-8   yes    -0.696291        4  -0.0915999  -1.73416     1.46002   DF ⋯\r\n 643 │ VF-10  yes    -0.795761        4  -0.0915999   0.308428    1.41951   DF\r\n 644 │ VF-11  yes    -0.646556        4  -0.0915999  -0.722065    1.46002   DF\r\n                                                  2 columns and 629 rows omitted\r\n\r\nFitting a model\r\nAs I briefly mentioned above, we can take advantage of the TuringGLM.jl package to specify our model. TuringGLM.jl uses a simple coding syntax, that is consistent with other Julia packages such as StatsModels.jl and MixedModels.jl, and it will return an instantiated Turing.jl model without us having to do too much work.\r\nLet’s say that we want to model water temperature (Wtemp) as a function of how much of the stream area is covered by objects (e.g. rocks, woody debris) (cover) and the day of the year that sampling took place (DOY). This was not the intent of the original study - I am using this as a case-study example only. We are not interested in the actual results here, rather, pay attention to the process and familiarizing yourself with the code and how to interpret the model fits.\r\nStep 1: Specify the model\r\nThere are two components to specifying a Bayesian model. The first component is the likelihood, which is basically the same as selecting which family to pick using a standard GLM (e.g. Gaussian, Poisson, ect…). The second component is specifying the priors (e.g. prior information from the literature or a pilot study), if we have any.\r\nThe model is specified using using the @formula macro and then specifying the dependent variable followed by a tilde ~ then the independent variables separated by a plus sign +, or * if an interaction term is required.\r\n\r\n# Step #1: Specify the model formula\r\n# - Response variable: Wtemp (water temperature)\r\n# - Predictor variable(s): cover (vegetation cover) + DOY (day of year)\r\nfm = @formula(Wtemp ~ cover + DOY)\r\n\r\n# Step 2: Fit the model \r\n# - The first argument is the formula object we specified above \r\n# - The second argument is the dataset containing the response and predictor variables\r\n# - Because we are specifying a linear regression, we do not need to specify any \r\n#   likelihood distribution (e.g. Poisson, Bernoulli, ect...)\r\nmodel = TuringGLM.turing_model(fm, salamanders)\r\n\r\nNotice that we haven’t given the model any information other than the raw data. In other words, we have not specified any priors. When this happens, TuringGLM.jl will take care of us, and specify what it calls “…state-of-the-art default priors, based on the literature and the Stan community…”.\r\nStep 2: Sampling the posterior\r\nThe next step is to draw samples from the posterior distribution. This allows us to calculate a distribution for each of the model parameters given the underlying data. The way we do this is to use the sample function from Turing.jl. There is a lot of fancy stuff going on in the background to draw the samples. You can take a look here if you want to dive into the nitty-gritty.\r\nFor today, we are going to keep it simple and use the default method, the No U-turn sampler, otherwise known as NUTS, to sample the posterior.\r\n\r\n# Extract parameter estimates using `sample` from 'Turing.jl'\r\n# - Use No U-Turn Sampleer (NUTS) with 2000 samples \r\n# - Sample from 4 Markov chains using multiple threads MCMCThreads()\r\nn_samples = 2000\r\nresults = Turing.sample(model, Turing.NUTS(), MCMCThreads(), n_samples, 4)\r\n\r\nStep 3: Evaluate the model fit\r\nThe next step is to perform some checks that the model fit was okay. This is roughly equivalent to performing residual diagnostics in a frequentist modelling framework (e.g. QQplots, fitted vs residual plots, ect…). There are two essential checks we must perform:\r\nTrace plots: These plots show us how well our model has converged on its parameter estimates. What we want to see is the parameter estimates remaining stable (i.e. no patterns or obvious slopes) across the range of posterior samples and the draws from each train to be well mixed.\r\n\r\nR-hat: This value is a numerical estimate of whether the chains we have inspected in the traceplots have converged or not. R-hat must be above 0.99 and below 1.01 for our model to be considered valid (Brooks & Gelman, 1998; Gelman & Rubin, 1992).\r\n\r\nLet’s take a look at our trace plots using the built-in traceplot function in MCMCChains.jl:\r\n\r\nMCMCChains.traceplot(results, legend = :outerright)\r\n\r\n\r\n\r\n\r\nInspecting the plots indicates that the chains are well mixed (i.e. all the different coloured squiggly lines are plotted over one another) and that the chains have converged on a certain range for each model parameter (i.e. the range of the y-axis spread is relatively consistent across the range of the x-axis). Happy days.\r\nNow let’s calculate R-hat:\r\n\r\n\r\nStatsBase.summarystats(results)\r\n\r\n\r\n\r\n\r\n\r\n\r\nAll our R-hat values for the different model parameters fall within the bounds of 0.99 and 1.01. Thank goodness. We can be quite happy that our model appears to have converged and the parameter estimates that we obtain appear to be stable.\r\nStep 4: Model inference\r\nThe last step is to perform model inference. To do this, we can calculate summary statistics from the posterior distributions of the different model parameters.\r\nThe first thing to do here is to plot the posterior distributions of the different model parameters.\r\n\r\nStatsPlots.plot(results, seriestype = :mixeddensity)\r\n\r\n\r\n\r\n\r\nThe two plots that we are interested in are the ones labelled B[1] (which is the slope of the first X covariate we specified in the model formula, namely: \\(\\beta\\)[cover]) and B[2] (which represents the slope for the second X covariate we specified in the model formula, namely: \\(\\beta\\)[DOY]). The posterior distribution for B[1] is centered around -0.16, and ranges between -0.04 and about 0.30. Roughly, this implies that the average change in water temperature associated with a one unit change in cover is a decrease of 0.15 degrees. Similarly, the posterior distribution for B[2] is centered around 0.08, and ranges between -0.03 and about 0.3. Roughly, this implies that the average change in water temperature associated with a one unit change in DOY is a increase of about 0.08 degrees.\r\nWe can calculate some more informative summary statistics that will allow us to summarise and interpret our results more succinctly. To do this, we can calculate credible intervals. These are basically what most ecologists interpet as confidence intervals using a frequentist modelling approach. There is an important distinction between the two definitions, however. For example, a 95% confidence interval tells us that if the experiment were repeated 100 times, we would reasonably expect the best parameter estimate to fall within the bounds of the confidence interval calculated 95% of times. This is not the same as a credible interval, which allows us to make a probabilistic statement about the treatment effect, as discussed above. Let’s illustrate with our example.\r\n\r\n# Calculate parameter estimates and credible intervals \r\n# - Read the 95% CI as the 97.5 - 2.5% columns \r\nStatistics.quantile(results)\r\n\r\n\r\n\r\n\r\nThe 95% credible interval for \\(\\beta\\)[cover] is -0.08 to -0.23. This means that there is a 95% chance that the change in water temperature associated with a one unit increase in cover is somewhere between a 0.08 to 0.23 degree decrease. Note that we know the effect is a decrease because of the negative sign of the parameter estimates and intervals. Also note, at no point have we ever calculated a p-value. We can infer that the cover variable was statistically significant, whatever that means, because the credible interval for \\(\\beta\\)[cover] did not include 0 (which would indicate no effect on water temperature).\r\nSimilarly, the 95% credible interval for \\(\\beta\\)[DOY] is 0.0003 to 0.14. This means that there is a 95% chance that the change in water temperature associated with a one unit increase in DOY is somewhere between a 0.0004 to 0.14 degree increase.\r\nCompare with frequentist approach\r\nLet’s now compare the Bayesian linear regression model we fitted above with an ordinary least squares (OLS) model.\r\n\r\nols = lm(@formula(Wtemp ~ cover + DOY), salamanders)\r\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\r\n\r\nWtemp ~ 1 + cover + DOY\r\n\r\nCoefficients:\r\n───────────────────────────────────────────────────────────────────────────────\r\n                    Coef.  Std. Error      t  Pr(>|t|)    Lower 95%   Upper 95%\r\n───────────────────────────────────────────────────────────────────────────────\r\n(Intercept)   2.71979e-12   0.0380319   0.00    1.0000  -0.0746821    0.0746821\r\ncover        -0.156278      0.0388947  -4.02    <1e-04  -0.232654    -0.0799011\r\nDOY           0.0748419     0.0382482   1.96    0.0508  -0.00026502   0.149949\r\n───────────────────────────────────────────────────────────────────────────────\r\n\r\nAs we touched on above, the estimates obtained from OLS indicate a single best estimate of the covariates effect on Y. We can see here that the \\(\\beta\\)[cover] = -0.15, with a 95% confidence interval of -0.08 to -0.23. These estimates are basically identical to the Bayesian estimates derived above (\\(\\beta\\)[cover] = -0.15, with a 95% credible interval of -0.08 to -0.23). Given that we didn’t provide any priors to our Bayesian model, this isn’t a huge surprise.\r\nConclusion\r\nLet’s leave it there for today. Bayesian data analysis is not easy, particularly for people like me who have been using frequentist modelling approaches for so long. However, I hope that today you have seen the power of using Bayesian models in your research. The real joy of this approach is the probabalistic statements that we can make when performing model inference and not relying on a binary p-value to assess statistical significance, but rather, we focus on estimating the magnitude of the effect. In later posts, we will cover more advanced Bayesian modelling strategies, including how to plot predictions from the model. Any feedback would be much appreciated.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-03-24-building-your-first-bayesian-glm-using-turingjl-in-julia/plot_preview.png",
    "last_modified": "2022-03-31T12:06:14+02:00",
    "input_file": {},
    "preview_width": 456,
    "preview_height": 234
  },
  {
    "path": "posts/2022-02-09-automating-model-fitting-in-r-using-functional-programming/",
    "title": "Automating model fitting in R using functional programming",
    "description": "Using functional programming to purrr your way through fitting many models",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology"
      }
    ],
    "date": "2022-02-09",
    "categories": [
      "R",
      "tidyverse",
      "DHARMa",
      "R markdown",
      "Data visualisation",
      "purrr",
      "Functional programming",
      "GLM",
      "Model diagnostics"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nData\r\nModel fitting\r\n\r\n\r\nBackground\r\nA few weeks ago, I came across the tweet below from Dr. Dani Rabaiotti lamenting about having to plot hundreds of diagnostics plots of fitted models to add to a supplementary file for a manuscript.\r\n\r\n\r\n\r\nIn this blogpost, we are going to learn how to automate the process of fitting many models in R and extracting model coefficients and model diagnostics plots. We will be using functional programming principles to create clean, efficient and fully automated code. Thankfully, R has a marvelous package called purrr that we can use to quite easily automate basically any iterative process.\r\nData\r\nLet’s load in the Salamanders dataset that is built-in to the glmmTMB package (Price et al. 2016). The dataset consists of counts (e.g. abundances) of different salamanders at 23 different sites, with each site sampled on 4 occassions. During each sampling event, a number of covariates were measured (e.g. water temperature [wtemp], day of year [doy], whether the site was affected by mining or not [mined], ect…). Please see Price et al. (2016) for further details, or use the help function ?Salamanders.\r\n\r\n\r\n# Load the Salamanders dataset from glmmTMB package \r\ndf <- glmmTMB::Salamanders %>%\r\n  janitor::clean_names()\r\nhead(df)\r\n\r\n\r\n  site mined      cover sample        dop       wtemp       doy spp\r\n1 VF-1   yes -1.4423172      1 -0.5956834 -1.22937861 -1.497003  GP\r\n2 VF-2   yes  0.2984104      1 -0.5956834  0.08476529 -1.497003  GP\r\n3 VF-3   yes  0.3978806      1 -1.1913668  1.01417627 -1.294467  GP\r\n4  R-1    no -0.4476157      1  0.0000000 -3.02335795 -2.712216  GP\r\n5  R-2    no  0.5968209      1  0.5956834 -0.14434533 -0.686860  GP\r\n6  R-3    no  1.3428470      1  0.5956834 -0.01466007 -0.686860  GP\r\n  count\r\n1     0\r\n2     0\r\n3     0\r\n4     2\r\n5     2\r\n6     1\r\n\r\nModel fitting\r\nStep 1: Specify the model formulae you want to fit\r\nThe first step in automating the model fitting process is to specify the different model structures that you want to fit. To simplify this process, we are going to specify only four models below. However, the exact same code can be used to fit potentially hundreds or thousands of models.\r\nThe four models we are going to fit are:\r\n- Model #1: Salamander counts as a function of water temperature (count ~ wtemp)\r\n- Model #2: Salamander counts as a function of the day of year (count ~ doy)\r\n- Model #3: Salamander counts as a function of water temperature and doy (count ~ wtemp + doy)\r\n- Model #4: Salamander counts as a function of an interaction between water temperature and doy (count ~ wtemp * doy)\r\nWe use the paste function to specify character strings containing the different model formulae we want to fit.\r\n\r\n\r\n# Specify model formulae\r\nformulae <- paste(\r\n  # Specify the response variable here\r\n  \"count ~\",\r\n  # Specify the X predictor variables to fit the each model here\r\n  # - Each line will be a different model\r\n  c(# Model #1: count ~ wtemp\r\n    \"wtemp\",\r\n    # Model #2: count ~ doy\r\n    \"doy\",\r\n    # Model #3: count ~ wtemp + doy\r\n    \"wtemp + doy\",\r\n    # Model #4: count ~ wtemp * doy\r\n    \"wtemp * doy\"))\r\nformulae\r\n\r\n\r\n[1] \"count ~ wtemp\"       \"count ~ doy\"         \"count ~ wtemp + doy\"\r\n[4] \"count ~ wtemp * doy\"\r\n\r\nStep 2: Iteratively fit models\r\nNow we get to use the marvelous map functions from the purrr package to iteratively fit many models. The map functions apply a function to each element of a list. In non-computer nerd terms, we use map to fit a GLM using each of the formulae we specified above. So, we are going to feed in the model formulae stored in formulae and then fit 4 different Poisson GLM’s.\r\n\r\n\r\n# Iteratively fit models \r\nmodels <- tibble(formulae) %>%\r\n  dplyr::mutate(# Run Poisson GLM's for each formula defined above in 'formulae'\r\n    mods = purrr::map(formulae,\r\n                      ~ glm(\r\n                        as.formula(.),\r\n                        family = poisson(link = \"log\"),\r\n                        data = df\r\n                      )))\r\nmodels\r\n\r\n\r\n# A tibble: 4 x 2\r\n  formulae            mods  \r\n  <chr>               <list>\r\n1 count ~ wtemp       <glm> \r\n2 count ~ doy         <glm> \r\n3 count ~ wtemp + doy <glm> \r\n4 count ~ wtemp * doy <glm> \r\n\r\nThe code above is equivalent to manually specifying each of the four different models, e.g. \r\nmod1 <- glm(count ~ wtemp, data = df, family = poisson(link = “log”))\r\nmod2 <- glm(count ~ doy, data = df, family = poisson(link = “log”))\r\nmod3 <- glm(count ~ wtemp + doy, data = df, family = poisson(link = “log”))\r\nmod4 <- glm(count ~ wtemp * doy, data = df, family = poisson(link = “log”))\r\nStep 3: Extract model results\r\nThe only problem with our code so far is that the GLM’s we fitted are stored in a list. Lists are not exactly the most intuitive objects to work with, in my opinion, at least not for beginner R users. So, the next step in our automation process is to iteratively extract the results and summary statistics for each model fit.\r\nAfter fitting our GLM’s, we can extract a range of model summary statistics for each model using broom::glance, which returns metrics such as: AIC, BIC, model deviance, the log likelihood [logLik], ect… I won’t go into much detail on how to use these statistics for model selection and validation. Very simply, the model with the lowest AIC value is typically selected as the best performing model. The output of broom::glance, just like the GLM models, are stored in a list. Boooo. The last thing we have to do is use tidyr::unnest to flatten the list back into a column in a dataframe.\r\n\r\n\r\n# Iteratively fit models and extract results \r\nmodels <- tibble(formulae) %>%\r\n  dplyr::mutate(# Run Poisson GLM's for each formula defined above in 'formulae'\r\n    mods = purrr::map(formulae,\r\n                      ~ glm(\r\n                        as.formula(.),\r\n                        family = poisson(link = \"log\"),\r\n                        data = df\r\n                      )),\r\n    # Extract results and summary statistics for each model\r\n    # - Note how we now pass 'purrr' the 'mods' column we created \r\n    #   containing a list of the models we have specified\r\n    # - We are no longer using the formulae, but the resulting models.\r\n    results = purrr::map(mods,\r\n                         ~ broom::glance(.))\r\n    ) %>%\r\n  # Convert the list of 'results' back into a dataframe column\r\n  tidyr::unnest(cols = c(\"results\"))\r\nmodels\r\n\r\n\r\n# A tibble: 4 x 10\r\n  formulae     mods  null.deviance df.null logLik   AIC   BIC deviance\r\n  <chr>        <lis>         <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>\r\n1 count ~ wte~ <glm>         2121.     643 -1418. 2841. 2850.    2114.\r\n2 count ~ doy  <glm>         2121.     643 -1420. 2844. 2853.    2116.\r\n3 count ~ wte~ <glm>         2121.     643 -1416. 2838. 2851.    2108.\r\n4 count ~ wte~ <glm>         2121.     643 -1409. 2827. 2845.    2095.\r\n# ... with 2 more variables: df.residual <int>, nobs <int>\r\n\r\nIf we were going to perform model selection/inference, we would say something like model #4, which modelled salamander counts as a function of water temperature and doy, including an interaction term, was the top-performing model. This model was selected as it had the lowest AIC and log likelihood value.\r\nStep 4: Plot model diagnostics\r\nJust because model #4 was the top-performing model from amongst the candidate models we fit, this doesn’t mean any of these models were a good fit to the data. To remedy this, we typically want to perform some type of residual analysis to assess our models fits. My go-to approach for plotting residuals from fitted models in R is to use the DHARMa package (Hartig 2022). Please consult the package vignette for details.\r\nBelow, let’s see how we can iteratively fit each of the four candidate models (as we have done previously), and then make residual plots for each model.\r\n\r\n\r\n# Iteratively fit models and extract results \r\nmodel_resids <- tibble(formulae) %>%\r\n  dplyr::mutate(# Run Poisson GLM's for each formula defined above in 'formulae'\r\n    mods = purrr::map(formulae,\r\n                      ~ glm(\r\n                        as.formula(.),\r\n                        family = poisson(link = \"log\"),\r\n                        data = df\r\n                      )),\r\n    # Extract DHARMa residuals plots for each model\r\n    # - Note how we now pass 'purrr' the 'mods' column we created \r\n    #   containing a list of the models we have specified\r\n    # - We are no longer using the formulae, but the resulting models \r\n    # - We specify 'plot = F' to not print each of the residual plots as the function runs\r\n    resid_plots = purrr::map(mods,\r\n                             ~ DHARMa::simulateResiduals(., plot = F)))\r\nmodel_resids\r\n\r\n\r\n# A tibble: 4 x 3\r\n  formulae            mods   resid_plots\r\n  <chr>               <list> <list>     \r\n1 count ~ wtemp       <glm>  <DHARMa>   \r\n2 count ~ doy         <glm>  <DHARMa>   \r\n3 count ~ wtemp + doy <glm>  <DHARMa>   \r\n4 count ~ wtemp * doy <glm>  <DHARMa>   \r\n\r\nThis is all good and well, but the plots that are produced aren’t exactly in a very usable form right now. We need to find a way to make the plots and then render them in a clean and organised fashion.\r\nStep 5: Store model diagnostics plots as a R markdown appendix\r\nLet’s make the diagnostics plots and then store them in a pdf that we could submit as a supplementary file for a manuscript or appendix using R markdown. In the code chunk below, we repeat much of what we have done already, with a few helpful functions to clean the results plots and headers to make the creation of our PDF a bit cleaner. See the code comments below for more details.\r\n\r\n\r\n# Iteratively fit models, make diagnostics plots and clean plot outputs\r\nmodels <- tibble(formulae) %>%\r\n  dplyr::mutate(\r\n    # Run Poisson GLM's for each formula defined above in 'formulae'\r\n    mods = purrr::map(formulae,\r\n                      ~ glm(\r\n                        as.formula(.),\r\n                        family = poisson(link = \"log\"),\r\n                        data = df\r\n                      )),\r\n    # Extract DHARMa residuals plots for each model\r\n    # - Note how we now pass 'purrr' the 'mods' column we created\r\n    #   containing a list of the models we have specified\r\n    # - We are no longer using the formulae, but the resulting models\r\n    resid_plots = purrr::map(mods,\r\n                             ~ DHARMa::simulateResiduals(., plot = F)),\r\n    # Extract AICc for each model\r\n    AICc = purrr::map_dbl(mods,\r\n                          ~ MuMIn::AICc(.))\r\n  ) %>%\r\n  # Calculate delta AICc\r\n  dplyr::ungroup() %>%\r\n  dplyr::mutate(deltaAICc = round(AICc - min(AICc), digits = 2)) %>%\r\n  # Process the model names to make the titles for each page cleaner\r\n  dplyr::group_by(formulae) %>%\r\n  dplyr::mutate(model_no = dplyr::cur_group_rows()) %>%\r\n  # Here, we paste a bunch of information from different columns in our\r\n  # dataset and character strings (e.g. \"...\") to create a title\r\n  # for each page with the model number, the model formula and the\r\n  # delta AICc value of the model\r\n  dplyr::mutate(formulae = paste0(\"Model #\",\r\n                                  model_no,\r\n                                  \": \",\r\n                                  formulae,\r\n                                  \" (Delta AICc = \",\r\n                                  deltaAICc,\r\n                                  \")\"))\r\nmodels\r\n\r\n\r\n# A tibble: 4 x 6\r\n# Groups:   formulae [4]\r\n  formulae                  mods  resid_plots  AICc deltaAICc model_no\r\n  <chr>                     <lis> <list>      <dbl>     <dbl>    <int>\r\n1 Model #1: count ~ wtemp ~ <glm> <DHARMa>    2841.      14.2        1\r\n2 Model #2: count ~ doy (D~ <glm> <DHARMa>    2844.      16.8        2\r\n3 Model #3: count ~ wtemp ~ <glm> <DHARMa>    2838.      11.0        3\r\n4 Model #4: count ~ wtemp ~ <glm> <DHARMa>    2827.       0          4\r\n\r\nThis code gives us our GLM’s (stored in mods column), our residuals plots (stored in resid_plots) and a title with some summary statistics to use as a title for each plot (stored in formulae).\r\nThe next step is to flatten the list of residual plots and then iteratively build our pdf, whereby each residual plot is saved on its own page and with its own title.\r\n\r\n\r\n# Automate the plotting of residual plots \r\nfor (i in 1:nrow(models)) {\r\n  \r\n  # Add a title for each page\r\n  # - The title is the character string defined in model$formulae\r\n  pander::pandoc.header(models$formulae[i], level = 3)\r\n  \r\n  # Add the plot for each page\r\n  plot(models$resid_plots[[i]]) \r\n  \r\n  # Add a pagebreak between each model plot\r\n  # - Note the extra '\\' needed to escape the '\\newpage' call function\r\n  pander::pandoc.p('\\\\newpage')\r\n  \r\n}\r\n\r\n\r\n\r\n\r\n\r\n\r\nAnd voila! We have a beautifully formatted pdf document with each of our residual plots and an associated informative title. I have provided a minimal R markdown template so you can make a beautiful supplementary file for your next manuscript submission. The template is available here and the example pdf file is here.\r\n\r\n\r\n\r\nHartig, Florian. 2022. DHARMa: Residual Diagnostics for Hierarchical (Multi-Level/Mixed) Regression Models. https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html.\r\n\r\n\r\nPrice, Steven J., Brenee’ L. Muncy, Simon J. Bonner, Andrea N. Drayer, and Christopher D. Barton. 2016. “Effects of Mountaintop Removal Mining and Valley Filling on the Occupancy and Abundance of Stream Salamanders.” Journal of Applied Ecology 53 (2): 459–68. https://doi.org/10.1111/1365-2664.12585.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-02-09-automating-model-fitting-in-r-using-functional-programming/screenshot_appendix1.png",
    "last_modified": "2022-02-10T11:25:44+02:00",
    "input_file": {},
    "preview_width": 961,
    "preview_height": 644
  },
  {
    "path": "posts/2022-01-15-import-and-combine-multiple-csv-files-using-r-and-julia/",
    "title": "Import and combine multiple .csv files using R and Julia",
    "description": "Tutorial on how to import and combine multiple .csv files into a single dataframe",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology"
      }
    ],
    "date": "2022-01-15",
    "categories": [
      "R",
      "Julia",
      "Data import",
      "purrr",
      "DataFrames.jl",
      "CSV.jl"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nUsing R\r\nOption #1: No need to save the filename in the dataframe\r\nOption #2: Save the filename as a column in the dataframe\r\n\r\nUsing Julia\r\n\r\n\r\n\r\n\r\n\r\nBackground\r\nIn this post, we are going to look at how to programmatically import multiple .csv files into a single dataframe using both R and Julia. This post is motivated by a number of recent consultations I have done, whereby the user has stored their data in an often large number of separate .csv files, with each file representing a different site, month, species or some of group. However, to analyse their data, the user requires all the .csv files to be imported into a single dataframe.\r\nBelow, I am going to demonstrate how to do this using both R and Julia. I will use a toy-example whereby I have three separate .csv files, each representing data collected data from a different site. While there are only three .csv files in this example, because we are importing and combining the data programmatically, we could have 100 or 1000, or 10000 .csv files and the code would be exactly the same.\r\nUsing R\r\nWe will use the fantastic purrr package in R to iteratively find each .csv file, import the data from each file, and then combine all of these files into a single dataframe.\r\n\r\n\r\n# Load required packages \r\nlibrary(tidyverse)\r\nlibrary(purrr)\r\n\r\n\r\n\r\nOption #1: No need to save the filename in the dataframe\r\nIn this first example, we do not need to add a column containing the file name. This option works when the .csv files have a column containing an appropriate identifier (e.g. a column containing site names). The process requires two steps:\r\nStep 1: Point R to the folder containing the .csv files to import using the built-in list.files function. We have to specify the path to the file on our computer using file.path. Here, the .csv files are stored in a folder called data_raw. If you aren’t using an R Project, you will need to specify the file path in full (e.g. C:/UserName/MyDocuments/...). We specify pattern = \".csv to tell R to only import the .csv files.\r\nStep 2: Use purrr::map_dfr to import all of the .csv files found in step 1 into a dataframe. Typically, purrr will import items into a list of objects. However, we explicitly use the purrr::map_dfr function to ensure our final data is stored in a data frame.\r\n\r\n\r\n# Programatically import and combine .csv files\r\ndata <-\r\n  # Step 1: Tell R where to find the .csv files you want to import\r\n  list.files(file.path(\"data_raw\"),\r\n             full.names = TRUE,\r\n             pattern = \".csv\") %>%\r\n  # Step 2: Import the files into a single dataframe\r\n  purrr::map_dfr(readr::read_csv)\r\n\r\n\r\n\r\nLet’s check that the data imported correctly…\r\n\r\n\r\n# Is the data structure a dataframe? \r\nclass(data)\r\n\r\n\r\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \r\n\r\n# Look at the first 6 rows \r\nhead(data)\r\n\r\n\r\n# A tibble: 6 x 2\r\n  site  count\r\n  <chr> <dbl>\r\n1 siteA     3\r\n2 siteA     6\r\n3 siteA     9\r\n4 siteB    20\r\n5 siteB    30\r\n6 siteB    40\r\n\r\nOption #2: Save the filename as a column in the dataframe\r\nIn this second example, we will add a column containing the filename. This option works when the .csv files do not have a column containing an appropriate identifier (e.g. a column containing site names), but rather an appropriate identifier is present in the filename (e.g. siteA.csv).\r\nThe process is similar to option #1 above, however we need to add two additional arguments. Firstly, we use purrr::set_names to store the filename that we are importing. Thereafter, we add the .id = \"filename argument to purrr::map_dfr to add a column called filename to our dataframe containing the filename.\r\n\r\n\r\n# Programatically import and combine .csv files\r\ndata_filename <-\r\n  # Tell R where to find the .csv files you want to import\r\n  list.files(file.path(\"data_raw\"),\r\n             full.names = TRUE,\r\n             pattern = \".csv\") %>%\r\n  # Store the filenames\r\n  purrr::set_names(nm = (basename(.))) %>%\r\n  # Import the files into a single dataframe with a column containing the filename\r\n  purrr::map_dfr(readr::read_csv, .id = \"filename\")\r\n\r\n\r\n\r\nLet’s see what that did.\r\n\r\n\r\n# Is the data structure a dataframe? \r\nclass(data_filename)\r\n\r\n\r\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \r\n\r\n# Look at the first 6 rows \r\nhead(data_filename)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  filename  site  count\r\n  <chr>     <chr> <dbl>\r\n1 siteA.csv siteA     3\r\n2 siteA.csv siteA     6\r\n3 siteA.csv siteA     9\r\n4 siteB.csv siteB    20\r\n5 siteB.csv siteB    30\r\n6 siteB.csv siteB    40\r\n\r\nNotice the new column called filename containing each of the filenames each dataset came from.\r\nThe only issue with this is that we probably don’t want to keep the .csv extension in the filename identifier. Let’s remove it using the built-in function tools::file_path_sans_ext.\r\n\r\n\r\n# Programatically import and combine .csv files\r\ndata_filename_ext <-\r\n  # Tell R where to find the .csv files you want to import\r\n  list.files(file.path(\"data_raw\"),\r\n             full.names = TRUE,\r\n             pattern = \".csv\") %>%\r\n  # Get filenames and remove the .csv extension code\r\n  purrr::set_names(nm = (basename(.) %>% tools::file_path_sans_ext())) %>%\r\n  # Import the files into a single dataframe with a column containing the filename\r\n  purrr::map_dfr(readr::read_csv, .id = \"filename\")\r\n\r\n\r\n\r\nLet’s see what that did.\r\n\r\n\r\n# Is the data structure a dataframe? \r\nclass(data_filename_ext)\r\n\r\n\r\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \r\n\r\n# Look at the first 6 rows \r\nhead(data_filename_ext)\r\n\r\n\r\n# A tibble: 6 x 3\r\n  filename site  count\r\n  <chr>    <chr> <dbl>\r\n1 siteA    siteA     3\r\n2 siteA    siteA     6\r\n3 siteA    siteA     9\r\n4 siteB    siteB    20\r\n5 siteB    siteB    30\r\n6 siteB    siteB    40\r\n\r\nUsing Julia\r\n\r\n# Install required packages, if required\r\n#using Pkg\r\n#Pkg.add(\"DataFrames\")\r\n#Pkg.add(\"CSV\")\r\n#Pkg.add(\"Glob\")\r\n\r\n# Load required packages (every new session)\r\nusing CSV             # Import and process .csv files \r\nusing Glob            # Find files on your PC\r\nusing DataFrames      # Work with dataframes\r\n\r\nThe first step is to point Julia to the folder containing the .csv files. Here we use the .\\ to indicate a relative file path (e.g. the ./ is the folder containing the project manifest - much like an R Project). Otherwise you can specify a full file path (e.g. e.g. C:/UserName/MyDocuments/...).\r\n\r\n# Specify file path to folder containg the .csv files \r\nfilepath = raw\".\\data_raw\"\r\n\".\\\\data_raw\"\r\n\r\nThe next step is to use Glob.jl to specify that we only want to import .csv files.\r\n\r\n# Search for the .csv files only\r\nfiles = Glob.glob(\"*.csv\", filepath)\r\n3-element Vector{String}:\r\n \".\\\\data_raw\\\\siteA.csv\"\r\n \".\\\\data_raw\\\\siteB.csv\"\r\n \".\\\\data_raw\\\\siteC.csv\"\r\n\r\nNow we can go ahead and actually import all the .csv files. Unlike the purrr::map_dfr function used earlier, each dataframe is imported as an individual element in an array, not the dataframe that we require.\r\n\r\n# Import .csv files (they are stored as an array of elements)\r\ndata_array = DataFrames.DataFrame.(CSV.File.(files))\r\n3-element Vector{DataFrame}:\r\n 3×2 DataFrame\r\n Row │ site    count\r\n     │ String  Int64\r\n─────┼───────────────\r\n   1 │ siteA       3\r\n   2 │ siteA       6\r\n   3 │ siteA       9\r\n 3×2 DataFrame\r\n Row │ site    count\r\n     │ String  Int64\r\n─────┼───────────────\r\n   1 │ siteB      20\r\n   2 │ siteB      30\r\n   3 │ siteB      40\r\n 3×2 DataFrame\r\n Row │ site    count\r\n     │ String  Int64\r\n─────┼───────────────\r\n   1 │ siteC     100\r\n   2 │ siteC     200\r\n   3 │ siteC     300\r\n\r\nFinally, we can convert the array of dataframes into a single dataframe.\r\n\r\n# Convert the array into a single dataframe \r\ndata_df = reduce(vcat, data_array)\r\n9×2 DataFrame\r\n Row │ site    count\r\n     │ String  Int64\r\n─────┼───────────────\r\n   1 │ siteA       3\r\n   2 │ siteA       6\r\n   3 │ siteA       9\r\n   4 │ siteB      20\r\n   5 │ siteB      30\r\n   6 │ siteB      40\r\n   7 │ siteC     100\r\n   8 │ siteC     200\r\n   9 │ siteC     300\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-15T14:49:47+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-14-species-accumulation-cures-in-r-part-1/",
    "title": "Species accumulation curves in R: Part 1",
    "description": "Basic species accumulation curve",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology"
      }
    ],
    "date": "2022-01-14",
    "categories": [
      "R",
      "vegan",
      "Species accumulation curves"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nData\r\nBasic species accumulation curve\r\nMake a plot\r\n\r\n\r\n\r\n\r\n\r\nBackground\r\nToday, we are going to look at how to compute and plot species accumulation curves (hereafter ‘SAC’) using the amazing vegan package. SAC’s are extremely popular in ecological analyses as they allow us to evaluate whether performing additional sampling is required to record all of the insect species associated with a particular plant species, for example.\r\nIn this session, we are going to cover the most basic SAC - observed richness. Observed species richness (hereafter ‘S’) simply tells us:\r\n* how many species we have recorded, to date, and\r\n* whether more surveys could yield new additional species.\r\n* But, it DOES NOT tell us how many species could be in the community (i.e. we cannot extrapolate) - we will cover how to extrapolate species richness in a later post.\r\nData\r\nLet’s load in a typical community richness dataset (species abundance matrix). This dataset represents the insect community associated with the shurb, Lycium ferocissimum, in South Africa.\r\n\r\n\r\n# Load required packages \r\nif (!require(\"pacman\")) install.packages(\"pacman\")\r\npacman::p_load(tidyverse, \r\n               tidyr, \r\n               janitor,\r\n               vegan)\r\n\r\n# Read in data \r\nsp_comm <- readr::read_csv(\"https://raw.githubusercontent.com/guysutton/CBC_coding_club/master/data_raw/species_abundance_matrix_ex.csv\") %>%\r\n  # Clean column names \r\n  janitor::clean_names()\r\n\r\n# Check data entry \r\ndplyr::glimpse(sp_comm)\r\n\r\n\r\nRows: 56\r\nColumns: 56\r\n$ provinces                                   <chr> \"Eastern Cape\", ~\r\n$ climatic_zones                              <chr> \"Cfb\", \"Cfb\", \"C~\r\n$ site                                        <chr> \"EC1\", \"EC1\", \"E~\r\n$ season                                      <dbl> 1, 2, 1, 2, 1, 2~\r\n$ haplotype                                   <dbl> 5, 5, 5, 5, 5, 5~\r\n$ cleta_eckloni                               <dbl> 23, 8, 11, 0, 0,~\r\n$ pseudambonea_capeni_schuhistes_lekkersingia <dbl> 4, 28, 0, 0, 0, ~\r\n$ acanthocoris_spinosus                       <dbl> 1, 0, 0, 0, 0, 0~\r\n$ antestiopsis_thunbergii                     <dbl> 1, 0, 0, 0, 0, 0~\r\n$ cassida_distinguenda                        <dbl> 0, 3, 0, 0, 0, 0~\r\n$ epilachna_sp_1                              <dbl> 0, 4, 0, 0, 0, 0~\r\n$ cleta_sp_1                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cleta_sp_2                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ exochomus_flavipes                          <dbl> 0, 0, 0, 0, 0, 0~\r\n$ scymnus_sp                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cheilomenes_lunata                          <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cheilomenes_sulphurea                       <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cf_nephus_sp                                <dbl> 0, 0, 0, 0, 0, 0~\r\n$ chnootriba_sp                               <dbl> 0, 0, 0, 0, 0, 0~\r\n$ oenopia_cinctella                           <dbl> 0, 0, 0, 0, 0, 0~\r\n$ hippodamia_variegate                        <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cassida_melanophthalma                      <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cassida_reticulipennis                      <dbl> 0, 0, 0, 0, 0, 0~\r\n$ macetes_sp                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cryptocephalus_nr_liturellus                <dbl> 0, 0, 0, 0, 0, 0~\r\n$ epitrix_sp                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ chrysomelidae_sp                            <dbl> 0, 0, 0, 0, 0, 0~\r\n$ sulcobruchus_longipennis                    <dbl> 0, 0, 0, 0, 0, 0~\r\n$ monolepta_bioculata                         <dbl> 0, 0, 0, 0, 0, 0~\r\n$ eurytomidae_sp                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ pachycnema_sp                               <dbl> 0, 0, 0, 0, 0, 0~\r\n$ scarabaeidae_sp                             <dbl> 0, 0, 0, 0, 0, 0~\r\n$ neoplatygaster_serieturberculata            <dbl> 0, 8, 0, 0, 0, 0~\r\n$ sciobius_sp                                 <dbl> 0, 0, 0, 0, 0, 0~\r\n$ lixini_sp                                   <dbl> 0, 0, 0, 0, 0, 0~\r\n$ beaufortiana_cornuta                        <dbl> 0, 0, 0, 0, 1, 0~\r\n$ pentatomidae_sp                             <dbl> 0, 0, 0, 0, 0, 0~\r\n$ apalochrus_sp                               <dbl> 0, 0, 0, 0, 0, 0~\r\n$ hylomela_sexpunctata                        <dbl> 0, 0, 0, 0, 1, 0~\r\n$ anthripidae_sp                              <dbl> 0, 0, 0, 0, 1, 0~\r\n$ cenaeus_carnifex                            <dbl> 0, 0, 0, 0, 0, 0~\r\n$ thrips_simplex                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ ceratitis_sp                                <dbl> 0, 0, 0, 0, 0, 0~\r\n$ brachymeria_sp                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ syrphidae_sp                                <dbl> 0, 0, 0, 0, 0, 0~\r\n$ cicadidae_sp                                <dbl> 0, 0, 0, 0, 0, 0~\r\n$ apis_mellifera                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ pteromalidae_sp                             <dbl> 0, 0, 0, 0, 0, 0~\r\n$ chrysopidae_sp                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ pamphagidae_sp                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ amphipsocidae_sp                            <dbl> 0, 0, 0, 0, 0, 0~\r\n$ diptera_sp                                  <dbl> 0, 0, 0, 0, 0, 0~\r\n$ hymenoptera_sp                              <dbl> 0, 0, 0, 0, 0, 0~\r\n$ decapotoma_lunata                           <dbl> 0, 0, 0, 0, 0, 0~\r\n$ pyrrhocordae_sp_1                           <dbl> 0, 0, 0, 0, 0, 0~\r\n$ mylabris_oculata                            <dbl> 0, 0, 0, 0, 0, 0~\r\n\r\nBasic species accumulation curve\r\nNow it is time to compute our species accumulation curve. To do this, we will use the poolaccum function from the vegan R package. Notice the first few columns are site description variables (i.e. not species abundances). We need to remove these columns, and only input the columns containing species abundances.\r\n\r\n\r\nsac_raw <- sp_comm %>%\r\n  # Remove site decsription variables \r\n  dplyr::select(-c(provinces, climatic_zones, site, season, haplotype)) %>%\r\n  # Compute SAC\r\n  vegan::poolaccum(.)\r\n\r\n\r\n\r\nWe now need to extract our observed species richness (S) estimates.\r\nN - No. of surveys (i.e. survey effort)\r\nS - Observed species richness\r\nlower2.5 - lower 95% confidence interval of S\r\nupper97.5 - upper 95% confidence interval of S\r\n\r\n\r\n# Extract observed richness (S) estimate \r\nobs <- data.frame(summary(sac_raw)$S, check.names = FALSE)\r\ncolnames(obs) <- c(\"N\", \"S\", \"lower2.5\", \"higher97.5\", \"std\")\r\nhead(obs)\r\n\r\n\r\n  N     S lower2.5 higher97.5      std\r\n1 3  7.44    0.475     15.525 3.537162\r\n2 4  9.57    2.475     17.000 3.682487\r\n3 5 11.62    4.425     19.525 3.770647\r\n4 6 13.26    6.000     19.525 3.683488\r\n5 7 14.95    9.000     22.000 3.780265\r\n6 8 16.73    9.000     24.525 3.935990\r\n\r\nMake a plot\r\nFinally, we can plot the desired species accumulation curve. Ultimately, we would like to see our S estimate and the 95% confidence intervals reach an asymptote (flat line) on the y-axis. This would indicate that performing additional surveys is highly unlikely to yield additional insects on the plant we have surveyed.\r\n\r\n\r\nobs %>%\r\n  ggplot(data = ., aes(x = N,\r\n                       y = S)) +\r\n  # Add confidence intervals\r\n  geom_ribbon(aes(ymin = lower2.5,\r\n                  ymax = higher97.5),\r\n              alpha = 0.5,\r\n              colour = \"gray70\") +\r\n  # Add observed richness line \r\n  geom_line() +\r\n  labs(x = \"No. of surveys\",\r\n       y = \"Observed richness\",\r\n       subtitle = \"More surveys are required to find all the insects on this plant\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-14-species-accumulation-cures-in-r-part-1/basic_sac_plot.png",
    "last_modified": "2022-01-14T17:45:18+02:00",
    "input_file": {},
    "preview_width": 4800,
    "preview_height": 3600
  },
  {
    "path": "posts/2021-09-17-adding-italics-to-ggplot-figures/",
    "title": "Adding italic species names to ggplot2 figures",
    "description": "How to add italics to ggplot figures",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology/"
      }
    ],
    "date": "2021-09-17",
    "categories": [
      "R",
      "tidyverse",
      "ggplot2",
      "ggtext",
      "R markdown",
      "PalmerPenguins",
      "Data visualisation"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nFigure 1\r\nStep 1: Plot basic figure\r\nStep 2: Set default ggplot theme\r\nStep 3: Changing axis labels and adding a title\r\nStep 4: Adding italics to title\r\n\r\nFigure 2\r\nStep 1: Plot basic figure\r\nStep 2: Add species names to x-axis labels\r\nStep 3: Move species name to a new line below common name\r\nStep 4: Make species names into italics\r\n\r\nConclusions\r\n\r\nBackground\r\nThe scared ecologist often finds themselves needing to make beautiful figures to include in their thesis or a new paper they want to submit to a high-profile journal. They open up R, load the ggplot2 package, make their figure, and save it to their PC… Voila! However, they realise that they actually needed to italicise the names of the species they work on. For example, consider Figure 4 in Towers and Dwyer, 2021, whereby the relationship between plant basal area and stem diameter are plotted for two different plant species (Acacia harpophylla and Casuarina cristata). This often results in many hours of trawling the internet trying to figure out how to do this in ggplot2, often with little success. They then resort to spending hours trying to annotate their plots in programmes such as InkScape or Adobe Illustrator.\r\nIn today’s blogpost, I am going to demonstrate a simple way to add italicised species names to your beautiful ggplot2 graphics, and make simple ggplot2 plots look a little neater, without needing to export your figures into other software.\r\nLet’s load the palmerpenguins dataset into R. This dataset contains body morphology measurements for a range of penguin species from the Antarctic islands (Gorman et al. 2014).\r\n\r\n\r\n# Load raw penguins data\r\npenguins <- palmerpenguins::penguins %>%\r\n  # Remove rows containing NA data\r\n  tidyr::drop_na(body_mass_g)\r\nhead(penguins)\r\n\r\n\r\n# A tibble: 6 x 8\r\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\r\n  <fct>   <fct>              <dbl>         <dbl>             <int>\r\n1 Adelie  Torgersen           39.1          18.7               181\r\n2 Adelie  Torgersen           39.5          17.4               186\r\n3 Adelie  Torgersen           40.3          18                 195\r\n4 Adelie  Torgersen           36.7          19.3               193\r\n5 Adelie  Torgersen           39.3          20.6               190\r\n6 Adelie  Torgersen           38.9          17.8               181\r\n# ... with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\r\n\r\nFigure 1\r\nLet’s assume we have analysed our dataset, and we now want to plot some figures. Let’s go ahead and make a basic figure, and we will incrementally make changes to the figure to make it publication ready.\r\nStep 1: Plot basic figure\r\nIn the first figure, we will plot the flipper length (in mm) on the x-axis and body mass (in g) on the y-axis, and colour the data points according to penguin species.\r\n\r\n\r\npenguins %>%\r\n  ggplot(data = ., aes(x = flipper_length_mm,\r\n                       y = body_mass_g,\r\n                       colour = species)) +\r\n  geom_point()\r\n\r\n\r\n\r\n\r\nStep 2: Set default ggplot theme\r\nThe first thing I recommend is changing the default ggplot2 theme to make the figures look a little neater and prettier (at least to my tastes!). There are a range of built-in themes that you can use (e.g. theme_classic(), theme_bw, ect…), but I like the theme below that I have manually written.\r\n\r\n\r\n# Change and set default ggplot theme\r\ntheme_set(theme_classic() +\r\n            theme(panel.border = element_rect(colour = \"black\", \r\n                                              fill = NA),\r\n                  axis.text = element_text(colour = \"black\"),\r\n                  axis.title.x = element_text(margin = unit(c(2, 0, 0, 0), \r\n                                                            \"mm\")),\r\n                  axis.title.y = element_text(margin = unit(c(0, 4, 0, 0), \r\n                                                            \"mm\"))))\r\n\r\n\r\n\r\nNow, remake the figure from above. Notice the difference in appearance? Whenever we make a new plot, the theme we set above will automatically be applied. Nifty.\r\n\r\n\r\npenguins %>%\r\n  ggplot(data = ., aes(x = flipper_length_mm,\r\n                       y = body_mass_g,\r\n                       colour = species)) +\r\n  geom_point()\r\n\r\n\r\n\r\n\r\nStep 3: Changing axis labels and adding a title\r\n\r\n\r\npenguins %>%\r\n  ggplot(data = ., aes(x = flipper_length_mm,\r\n                       y = body_mass_g,\r\n                       colour = species)) +\r\n  geom_point() +\r\n  # Change the labels of axes and legend\r\n  labs(\r\n    x = \"Flipper length (mm)\",\r\n    y = \"Body mass (g)\",\r\n    colour = \"Penguin species\",\r\n    title = \"Do larger penguins (Pygoscelis spp.) weigh more?\"\r\n  )\r\n\r\n\r\n\r\n\r\nStep 4: Adding italics to title\r\nWhile we have always been able to add italics to a ggplot2 figure, the code has always been pretty messy and confusing, in my opinion. It would take me many attempts to finally get the italics in the correct place… Maybe it is just me, but I don’t think so…\r\nEnter the amazing ggtext package. This package allows us to use markdown language and/or HTML to edit our ggplot2 figures by using the function element_markdown(). If you don’t know any markdown, don’t worry. There are many nice introductions to markdown, but even better, to italise any text in our ggplot2 figure, we simply need to put that text within a * either side of the word or phrase. For example, to make the word ‘Panthera’ italised, we need to type *Panthera*.\r\nLet’s test it out and make the genus name for our penguins (Pygoscelis) italicised in our figure.\r\n\r\n\r\npenguins %>%\r\n  ggplot(data = ., aes(x = flipper_length_mm,\r\n                       y = body_mass_g,\r\n                       colour = species)) +\r\n  geom_point() +\r\n  # Change the labels of axes and legend\r\n  labs(\r\n    x = \"Flipper length (mm)\",\r\n    y = \"Body mass (g)\",\r\n    colour = \"Penguin species\",\r\n    # We place * ___ * around the word we want italicised\r\n    title = \"Do larger penguins (*Pygoscelis* spp.) weigh more?\"\r\n  ) +\r\n  # Use ggtext to specify that the axis title should be interpreted as markdown text\r\n  theme(\r\n    plot.title = ggtext::element_markdown()\r\n  )\r\n\r\n\r\n\r\n\r\nThis looks like a pretty good figure to me. Let’s set this aside and pat ourselves on the back for a figure well made. :)\r\nFigure 2\r\nLet’s now make a second figure to demonstrate the power of element_markdown() and demonstrate how to add italicised text to our axis labels and figure legend. As above, we will go ahead and make a basic figure, and then incrementally make changes to the figure to make it publication ready.\r\nStep 1: Plot basic figure\r\nIn this second figure, we will plot the body mass (in g) on the y-axis and the penguin species on the x-axis, and colour the data points according to penguin species. We will use the scale_fill_grey function to colour the species-specific boxplots in greyscale. The start and end options specified within scale_fill_grey below make the shades lightgrey instead of dark grey/black (if you want darker shades, change this to: start = 0.2, end = 0.6).\r\n\r\n\r\n# Make a basic graph\r\npenguins %>%\r\n  ggplot(data = ., aes(x = species,\r\n                       y = body_mass_g,\r\n                       fill = species)) +\r\n  geom_boxplot() +\r\n  # Make the boxplots different shades of grey\r\n  scale_fill_grey(start = 0.5, end = 0.9) +\r\n  labs(\r\n    x = NULL,\r\n    y = \"Body mass (g)\",\r\n    fill = \"Penguin species\",\r\n  ) \r\n\r\n\r\n\r\n\r\nStep 2: Add species names to x-axis labels\r\nNow, let’s add the genus and species names to the x-axis along with the common name of each penguin. For example, the species name of the Adelie penguin is Pygoscelis adeliae or P. adeliae, the Chinstrap penguin is P. antarcticus, and the Gentoo penguin is P. papua. There are many ways to do this. We are going to manually change the names in the raw data before plotting (you could also use scale_x_discrete, if you really wanted).\r\n\r\n\r\n# Now italicise and place species on own line (with fill)\r\npenguins %>%\r\n  # Here, we loop through the species names, and if the species name == x, change it to the name given to the right of ~\r\n  dplyr::mutate(species = dplyr::case_when(\r\n    # e.g. If species = 'Adelie', change to 'Adelie (P. adeliae)'\r\n    species == \"Adelie\" ~ \"Adelie (P. adeliae)\",\r\n    species == \"Chinstrap\" ~ \"Chinstrap (P. antarcticus)\",\r\n    species == \"Gentoo\" ~ \"Gentoo (P. papua)\"\r\n  )) %>%\r\n  ggplot(data = ., aes(x = species,\r\n                       y = body_mass_g,\r\n                       fill = species)) +\r\n  geom_boxplot() +\r\n  scale_fill_grey(start = 0.5, end = 0.9) +\r\n  labs(\r\n    x = NULL,\r\n    y = \"Body mass (g)\",\r\n    fill = \"Penguin species\"\r\n  )\r\n\r\n\r\n\r\n\r\nStep 3: Move species name to a new line below common name\r\nThe x-axis labels look a bit silly in the above graph. It looks so messy having the common and species names on the same line. Let’s move the species name to its own line below the common name in the x-axis label. Welcome back element_markdown().\r\nWe are going to leverage the power of using element_markdown() to use HTML to insert a linebreak between the common name and species name. To insert a linebreak in HTML, we just need to add <br> wherever we want the linebreak.\r\n\r\n\r\n# Use HTML linebreaks to split common and species names in x-axis labels\r\npenguins %>%\r\n  # Add <br> where you want the linebreak to occur\r\n  dplyr::mutate(species = dplyr::case_when(\r\n    species == \"Adelie\" ~ \"Adelie <br> (P. adeliae)\",\r\n    species == \"Chinstrap\" ~ \"Chinstrap <br> (P. antarcticus)\",\r\n    species == \"Gentoo\" ~ \"Gentoo <br> (P. papua)\"\r\n  )) %>%\r\n  ggplot(data = ., aes(x = species,\r\n                       y = body_mass_g,\r\n                       fill = species)) +\r\n  geom_boxplot() +\r\n  scale_fill_grey(start = 0.5, end = 0.9) +\r\n  labs(\r\n    x = NULL,\r\n    y = \"Body mass (g)\",\r\n    fill = \"Penguin species\"\r\n  ) +\r\n  # Use ggtext to specify that the x-axis text and legend text should be interpreted as markdown text\r\n  theme(\r\n    axis.text.x = element_markdown(),\r\n    legend.text = element_markdown()\r\n  )\r\n\r\n\r\n\r\n\r\nStep 4: Make species names into italics\r\nMuch like for Figure 1, we can now make the species names italicised using * ___ * and element_markdown().\r\n\r\n\r\n# Make species names italicised \r\npenguins %>%\r\n  # Remember, to make a word/phrase italic, put it in * _____ * \r\n  dplyr::mutate(species = dplyr::case_when(\r\n    species == \"Adelie\" ~ \"Adelie <br> (*P. adeliae*)\",\r\n    species == \"Chinstrap\" ~ \"Chinstrap <br> (*P. antarcticus*)\",\r\n    species == \"Gentoo\" ~ \"Gentoo <br> (*P. papua*)\"\r\n  )) %>%\r\n  ggplot(data = ., aes(x = species,\r\n                       y = body_mass_g,\r\n                       fill = species)) +\r\n  geom_boxplot() +\r\n  scale_fill_grey(start = 0.5, end = 0.9) +\r\n  labs(\r\n    x = NULL,\r\n    y = \"Body mass (g)\",\r\n    fill = \"Penguin species\"\r\n  ) +\r\n  # Use ggtext to specify that the x-axis text and legend text should be interpreted as markdown text\r\n  theme(\r\n    axis.text.x = element_markdown(),\r\n    legend.text = element_markdown()\r\n  ) +\r\n  # Increase the space between legend items (looks a bit nicer)\r\n  theme(\r\n    legend.key.height = unit(1.2, 'cm')\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConclusions\r\nThere we go. The amazing ggtext package makes it really easy to use simple markdown and HTML to quickly add italicised species names to our ggplot2 figures and make them publication-ready quality without the need for editing figures in other software programmes. Are there other editing steps that you would performing outside of R to make your figures ready? Let me know and I am sure we can find a way to automate the process in R.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-09-17-adding-italics-to-ggplot-figures/adding-italics-to-ggplot-figures_files/figure-html5/figure2_italics-1.png",
    "last_modified": "2021-09-30T11:10:07+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-11-gaussian-glm-in-julia/",
    "title": "Gaussian GLM in Julia",
    "description": "A Julia alternative for fitting a simple GLM",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/stats_ecology"
      }
    ],
    "date": "2021-08-11",
    "categories": [
      "Julia",
      "GLM",
      "Gadfly",
      "Model diagnostics",
      "Hypothesis testing"
    ],
    "contents": "\r\n\r\nContents\r\nLoading and installing packages\r\n(i) Installing packages\r\n(ii) Loading packages\r\n\r\nCase study\r\n(i) Import data\r\n(ii) Study question\r\n\r\nExploratory data analysis\r\n(i) Check the data corresponds to the experimental design\r\n(ii) Visualise the data\r\n\r\nFitting the model\r\nEvaluating model fit\r\n(i) Plot #1: QQPlot\r\n(ii) Plot #2: Residuals vs Fitted Plot\r\n(iii) Plot #3: Scale-location plot\r\n\r\nStatistical inference\r\nConclusions\r\n\r\nIn the first post on this blog, we saw how to perform and interpret a simple General Linear Model (GLM) using a Gaussian (normal) distribution using R. Today, I am going to demonstrate how to perform the same analysis using Julia. Julia is a relatively new, really exciting programming language that is becoming more popular with biologists and ecologists due to its flexibility and speed. I find myself transitioning my data analyses from R to Julia quite frequently, and encourage you to explore Julia and whether it is a good fit for you and your work. Just remember, R is not the be-all-and-end-all of ecological statistics…\r\nLoading and installing packages\r\nI find loading and installing packages in Julia more intuitive than R, although there are many similiarities.\r\n(i) Installing packages\r\nFirstly, we need to install the packages we are going to use. This is equivalent to using install.packages(\"package_name\") in R. We only do this the first time we run our Julia script. Just remove the preceding # from each line below to install the packages, if necessary.\r\n\r\n# Load the package manager\r\nusing Pkg\r\n\r\n# Install required packages (once-off)\r\n# Pkg.add(\"GLM\")             # General linear models (GLM)\r\n# Pkg.add(\"StatsModels\")     # Perform likelihood-ratio test\r\n# Pkg.add(\"DataFrames\")      # Manipulating data structures (Julia's version of 'dplyr')\r\n# Pkg.add(\"DataFramesMeta\")  # Manipulating data structures (Julia's version of 'dplyr')\r\n# Pkg.add(\"Gadfly\")          # Produce nice figures (Julia's version of 'ggplot2')\r\n# Pkg.add(\"StatsBase\")       # Basic statistical functions (e.g. mean, stddev)\r\n# Pkg.add(\"Statistics\")      # More basic statistical functions \r\n# Pkg.add(\"Distributions\")   # Fit basic statistical distributions \r\n# Pkg.add(\"CSV\")             # Import and process .csv files\r\n# Pkg.add(\"Plots\")           # Plot figures \r\n# Pkg.add(\"StatsPlots\")      # Grouped box-plots recipe\r\n# Pkg.add(\"Cairo\")           # Save .png file\r\n# Pkg.add(\"Fontconfig\")      # Save .png file\r\n\r\n(ii) Loading packages\r\nSecondly, we need to load each of the packages required to perform our analyses. This is equivalent to using library(\"package_name\") in R. We have to load the packages each time we start a new Julia session.\r\n\r\n# Load required packages (every new session)\r\nusing GLM             # General linear models (GLM)\r\nusing StatsModels     # Perform likelihood-ratio test\r\nusing DataFrames      # Manipulating data structures (Julia's version of 'dplyr')\r\nusing DataFramesMeta  # Manipulating data structures (Julia's version of 'dplyr')\r\nusing Gadfly          # Produce nice figures (Julia's version of 'ggplot2')\r\nusing StatsBase       # Basic statistical functions (e.g. mean, stddev)\r\nusing Statistics      # More basic statistical functions \r\nusing Distributions   # Fit basic statistical distributions \r\nusing CSV             # Import and process .csv files\r\nusing Plots           # Plot figures \r\nusing StatsPlots      # Grouped box-plots recipe\r\nusing Cairo           # Save .png file \r\nusing Fontconfig      # Save .png file\r\n\r\nCase study\r\n(i) Import data\r\nWe are going to use the same data for the the previous analysis performed in R. The study measured the body mass of 30 mussels (body_mass) from two field sites (site). Here, we have a single categorical predictor variable (site). The example .csv file is available from my GitHub repository, which we are going to download using Julia below.\r\n\r\n# Provide the HTML address of the .csv file \r\nhtml = \"https://raw.githubusercontent.com/guysutton/stats_for_scared_ecologists/main/_posts/2021-08-11-gaussian-glm-in-julia/data_raw/data_mussel_body_mass.csv\";\r\n\r\n# Read in the HTML file and store is as a data frame \r\ndf = CSV.read(download(html), DataFrame)\r\n60×2 DataFrame\r\n Row │ body_mass  site\r\n     │ Float64    String\r\n─────┼───────────────────\r\n   1 │ 18.7754    A\r\n   2 │ 25.5246    A\r\n   3 │ 23.4865    A\r\n   4 │ 23.5963    A\r\n   5 │ 28.9805    A\r\n   6 │  0.774305  A\r\n   7 │ 22.6174    A\r\n   8 │ 29.1557    A\r\n  ⋮  │     ⋮        ⋮\r\n  54 │ 31.3118    B\r\n  55 │ 25.3978    B\r\n  56 │ 36.0377    B\r\n  57 │ 39.2729    B\r\n  58 │ 33.2952    B\r\n  59 │ 19.5086    B\r\n  60 │ 19.944     B\r\n          45 rows omitted\r\n\r\n(ii) Study question\r\nWe want to evaluate whether the mass of the mussels we collected differs between the two field sites.\r\nH0: Mussel body mass is not statistically different between the two sites.\r\nH1: Mussel body mass is statistically different between the two sites.\r\nExploratory data analysis\r\n(i) Check the data corresponds to the experimental design\r\nThe first thing I usually do is check that the number of replicates per treatment is in line with the experimental design. In other words, we sampled 30 mussels per site, so let’s just check that each site has 30 mussel body mass measurements.\r\n\r\n# Count the number of body mass measurements per site \r\ncombine(groupby(df, [:site]), nrow => :count)\r\n2×2 DataFrame\r\n Row │ site    count\r\n     │ String  Int64\r\n─────┼───────────────\r\n   1 │ A          30\r\n   2 │ B          30\r\n\r\nAs expected, there are 30 rows of data per site. Good start!\r\n(ii) Visualise the data\r\nThe next step is to plot the data. You should be noting aspects such as:\r\nIs there a visual difference in means or the distributions of values in the different groups?\r\nIs there more variation in one group vs another group?\r\n\r\n# Plot distribution of mussel body masses between sites\r\n@df df boxplot!(:site, \r\n                :body_mass, \r\n                fillalpha = 0.75, \r\n                linewidth = 2,\r\n                # Print legend or not\r\n                legend = false, \r\n                xlabel = \"Site\", \r\n                ylabel = \"Mussel body mass (g)\")\r\n\r\n\r\nIn this case, it looks like the median body mass (indicated by the bold black line) of mussels collected at site A is lower than than at site B). The variance (i.e. spread of body mass values along the y-axis) appears to be relatively similar between the two sites.\r\nFitting the model\r\nFitting a GLM is relatively simple in Julia. All we need to do is tell it what is our response variable (the response variable is the measurement we are interested in). Here, the response variable is (body_mass). We then specify our predictor variables to the right-hand side of this weird ~ (tilde) symbol. Our predictor variables are things we have recorded that we believe could be affecting the response variable. Here, our predictor variable was site. We need to tell Julia where these data are stored (df), and that we want a Gaussian GLM (Normal()), with an identity link function (GLM.IdentityLink).\r\nThe last argument we specify is contrast, which allows us to mainly specify the comparisons between categorical variables that we want to make. In this example, we are simply comparing measurements from site_A vs site_B, so effects coding is not overly important. However, this will become more important when we build more complex models in later blog posts (e.g. including interaction terms). I won’t go into detail here, other than to point you towards a nice introduction to effects coding that you can read in the meanwhile.\r\n\r\n# Fit Gaussian GLM\r\nm1 = fit(\r\n         # We want a GLM \r\n         GeneralizedLinearModel,\r\n         # Specify the model formula\r\n         @formula(body_mass ~ site),\r\n         # Where is the data stored? \r\n         df,\r\n         # Which statistical distribution should we fit?\r\n         Normal(),\r\n         # Which link function should be applied?\r\n         GLM.IdentityLink(),\r\n         # Specify effects coding (more on this in later posts)\r\n         contrasts = Dict(:site => EffectsCoding()));\r\n\r\nEvaluating model fit\r\nBefore we look at the results from our model, we must first check whether the GLM that we fit was an appropriate choice for our data. I won’t repeat the details from my analogous R post on the rationale and details behind model diagnostics. I haven’t managed to find an easy and intuitive package to perform model diagnostics in Julia (such as the DHARMa package in R). So below I have shown how to manually produce residuals diagnostics plots in Julia. I am working on writing up these methods as a new Julia package - watch this space!\r\n(i) Plot #1: QQPlot\r\nThe QQ plot tells us whether our data conforms to the distribution we specified in the in the GLM call above (remember: we said Normal()). If our GLM is a good fit to the data, the blue dots will fall approximately on the blue dashed 1:1 line.\r\n\r\n# Extract fitted/predicted values from model object\r\npred = GLM.predict(m1);\r\n\r\n# Extract vector of response values \r\nresp = df[!, :body_mass];\r\n\r\n# Calculate raw residuals \r\nresids = resp .- pred;\r\n\r\n# Calculate standard deviation of residuals\r\nsdResids = Statistics.std(resids);\r\n\r\n# Calculate length (n) of residuals vector\r\nnResids = Base.length(resids);\r\n\r\n# Calculate standard error of residuals\r\nseResids = sdResids / sqrt(nResids);\r\n\r\n# Calculate Standardised/studentized Pearson residuals\r\n# - These are equivalent to plot(model, which = 2) in R \r\nstzPearsResids = (resids / seResids) ./ 10;\r\n\r\n# Define quantiles\r\nqx = Distributions.quantile.(Distributions.Normal(),\r\n                                range(0.5,\r\n                                stop = (nResids .- 0.5),\r\n                                length = (nResids))    \r\n                            ./ (nResids .+ 1));\r\n                             \r\n# Create plot\r\np = Gadfly.plot(\r\n  # Add points layer\r\n  Gadfly.layer(x = qx,\r\n               y = sort(stzPearsResids),\r\n               Gadfly.Geom.point),\r\n  # Add 1:1 line\r\n  Gadfly.layer(x = [-3,3],\r\n               y = [-3,3],\r\n               Gadfly.Geom.line,\r\n               Gadfly.style(line_style = [:dot])),\r\n  # Change plot aesthetics\r\n  Gadfly.Guide.title(\"Normal Q-Q plot\"),\r\n  Gadfly.Guide.xlabel(\"Theoretical Quantiles\"),\r\n  Gadfly.Guide.ylabel(\"Std. Pearson residuals\"));\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe blue dots (representing model residuals) fall approximately along the blue dashed 1:1 line, indicating that our residuals approximate the Gaussian or Normal distribution that we specified during the model fitting process. As such, it is reasonable to assume that a Gaussian/Normal distribution was an appropriate choice.\r\n(ii) Plot #2: Residuals vs Fitted Plot\r\nThe residual vs fitted plot allows us to assess whether we satsify the assumption that our response variable (here: body_mass) can be modeled as a linear function of our predictor variable(s) (here: site), given we used a General LINEAR model. When our predictor variables are categorical, such as in the current model (remember site was our predictor variable with two levels, siteA and siteB), this plot will show dotplots with each level (e.g. each site) getting its own column. We want to see equally spread residuals around an approximately horizontal line, with no discernable pattern in the residuals.\r\n\r\n# Extract fitted/predicted values from model object\r\npred = GLM.predict(m1);\r\n\r\n# Extract vector containing response variable \r\nresponse_y = df[!, :body_mass];\r\n\r\n# Calculate deviance residuals \r\ndevResids = sign.(response_y .- pred) .* sqrt.(GLM.devresid.(Distributions.Normal(), \r\n                                                             response_y, \r\n                                                             pred));\r\n# Create plot\r\np = Gadfly.plot(\r\n            # Add points layer\r\n            x = pred,\r\n            y = devResids,\r\n            Gadfly.Geom.point,\r\n            Gadfly.layer(Gadfly.Geom.smooth(method = :loess,\r\n                                            smoothing = 0.9)),\r\n            # Change plot aesthetics\r\n            Gadfly.Guide.xlabel(\"Fitted values\", orientation=:horizontal),\r\n            Gadfly.Guide.ylabel(\"Residuals\"),\r\n            Gadfly.Guide.title(\"Residuals vs Fitted\"));\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHere, we see no significant pattern in the residuals. The residuals are approximately evenly distributed in the two different groups and are centered around a horizontal line of y = 0. This plots indicates that it is probably acceptable to assume a linear relationship between body_mass ~ site.\r\n(iii) Plot #3: Scale-location plot\r\nThis plots allows us to evaluate whether our data demonstrates heteroscedasticity. This is just a fancy way to say that the variance in the data is systematically dependent on some variable in the model. If you remember back to undergraduate statistics courses, linear models typically assume that the data displays equal variances across groups or numeric variables. When our predictor variables are categorical, this plot will show dotplots with each level (e.g. each site) getting its own dotplot.\r\nMuch like for the Residual vs Fitted Plot above, we want to see approximately equally distributed residuals across the two dotplots, and a horizontal y-line.\r\n\r\n# Calculate square root of Std. Pearson residuals \r\nstzPearsResids = (resids / seResids) ./ 10;\r\nsqrPearResids = sqrt.(complex(stzPearsResids));\r\nsqrPearResids = abs.(sqrPearResids);\r\n\r\n# Create plot\r\np = Gadfly.plot(x = pred, \r\n                y = sqrPearResids,\r\n                Gadfly.layer(Gadfly.Geom.point),\r\n                Gadfly.layer(Gadfly.Geom.smooth(method = :loess,\r\n                                                smoothing = 0.9)),\r\n                # Change plot aesthetics\r\n                Gadfly.Guide.title(\"Scale-Location\"),\r\n                Gadfly.Guide.xlabel(\"Predicted values\",\r\n                                    orientation=:horizontal),\r\n                Gadfly.Guide.ylabel(\"√ Std. Pearson residuals\"));\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe residuals are approximately evenly distributed in the two different groups, and the slope of the line between the two groups is approximately 0. Taken together, this plots indicates that the assumption of equality of variances has been met.\r\nStatistical inference\r\nNow to the bit of the analysis that most ecologists are most interested in (at least to appease their reviewers: assessing statistical significance and calculating p-values). Here, we perform statistical inference, which basically means we are going to evaluate “which [model] coefficients are non-zero beyond a reasonable doubt, implying meaningful associations between covariates and the response?” (Tredennick et al. 2021).\r\nTo do this, we will use a Likelihood Ratio Test (LRT). The LRT allows us to compare a model of interest with a null model that lacks some feature of the model of interest (Tredennick et al. 2021). For example, below we will compare our model of interest (containing the site predictor variable) with a null model which lacks the site predictor. This way we can ask whether adding information about site improved the likelihood of the data in comparison to the null model. Please see (Tredennick et al. 2021) for an excellent introduction to using the LRT for inference.\r\nBelow, we define our null model as model containing only a random-intercept only, which is probably the most common null model for basic statistical inference in ecology.\r\n\r\n# Define null model to assess effect of 'site' on mussel body mass\r\n# - Fit a random intercept only (indicated by y ~ 1)\r\nmnull = fit(\r\n         GeneralizedLinearModel,\r\n         @formula(body_mass ~ 1),\r\n         df,\r\n         Normal(),\r\n         GLM.IdentityLink());\r\n\r\nAnd finally, we can perform a LRT to test that hypothesis that mussel body mass differed between the two field sites sampled.\r\n\r\n# Perform LRT \r\nStatsModels.lrtest(mnull, m1)\r\nLikelihood-ratio test: 2 models fitted on 60 observations\r\n────────────────────────────────────────────────\r\n     DOF  ΔDOF   Deviance   ΔDeviance  p(>Chisq)\r\n────────────────────────────────────────────────\r\n[1]    2        8481.0951                       \r\n[2]    3     1  6983.1276  -1497.9675     <1e-99\r\n────────────────────────────────────────────────\r\n\r\nSo, the LRT tells us that including site as a predictor significantly improved the fit of our model to the data relative to the null model (\\(\\chi\\)2 = 1498, d.f. = 1, P < 0.001). Note how the results are exactly the same as we got using R previously. As they should be, but it is good to check! The LRT supports our alternative hypothesis that mussel body masses were different between the two field sites sampled.\r\nConclusions\r\nWe have now covered how to run and interpret a Gaussian GLM in both R and Julia. Please let me know if there are any improvements or what topics you would like me to cover going forward.\r\n\r\n\r\n\r\nTredennick, Andrew T, Giles Hooker, Stephen P Ellner, and Peter B Adler. 2021. “A Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology.” Ecology 102 (6): e03336.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-11-gaussian-glm-in-julia/mussel_qqplot.png",
    "last_modified": "2021-08-19T19:09:48+02:00",
    "input_file": {},
    "preview_width": 480,
    "preview_height": 480
  },
  {
    "path": "posts/2021-08-04-gaussian-general-linear-model-glm/",
    "title": "General Linear Model (GLM): Gaussian GLM",
    "description": "A Gentle Introduction",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/Guy_F_Sutton"
      }
    ],
    "date": "2021-08-04",
    "categories": [
      "R",
      "tidyverse",
      "GLM",
      "ggplot2",
      "Model diagnostics",
      "DHARMa",
      "Hypothesis testing"
    ],
    "contents": "\r\n\r\nContents\r\nGeneral linear models (GLM’s)\r\nBackground\r\nThe data\r\nStudy questions\r\n\r\nExploratory data analysis\r\n(i) Check the data corresponds to the experimental design\r\n(ii) Visualise the data\r\n\r\nFitting the model\r\nEvaluating model fit\r\n(i) Plot #1: QQPlot\r\n(ii) Plot #2: Residuals vs Predicted Plot\r\n\r\nStatistical inference\r\nMake a figure\r\nConclusions\r\n\r\nGeneral linear models (GLM’s)\r\nBackground\r\nMany scared ecologists worry about which statistical test to use to analyse their hard-earned data. This is natural. Typically, some form of linear model will be used to analyse the data, whereby the relationship between a continuous response variable (y) is modelled with respect to one or more explanatory variables (x1, x2, …). A general linear model (GLM) is an umbrella term for a linear model for data (residuals) that follow a normal or Gaussian distribution (more on this later). Basically, the GLM encompasses those pesky analyses you were likely taught during undergraduate statistics courses.\r\nA GLM where our x is numeric is analogous to a linear regression\r\nA GLM where our x is categorical is analogous to a analysis of variance (ANOVA)\r\nToday, we are going to work through a simple example of a Gaussian General Linear Model (GLM) with a single categorical x variable (i.e. ANOVA) using R.\r\nThe data\r\nLet’s consider a study where we measure the body mass of 30 mussels (body_mass) from two field sites (site). Here, we have a single categorical predictor variable (site).\r\n\r\n\r\n# Simulate data to correlate X and Y amongst groups \r\nset.seed(2021)             \r\ndata1 <- data.frame(y = rnorm(n = 30, mean = 20, sd = 10),\r\n                    x = rep(LETTERS[1], each = 30))\r\ndata2 <- data.frame(y = rnorm(n = 30, mean = 35, sd = 10),\r\n                    x = rep(LETTERS[2], each = 30))\r\ndata <- dplyr::bind_rows(data1, data2) %>%\r\n  dplyr::rename(body_mass = y,\r\n                site = x)\r\n\r\n\r\n\r\n\r\n\r\ndplyr::glimpse(data)\r\n\r\n\r\nRows: 60\r\nColumns: 2\r\n$ body_mass <dbl> 18.7754002, 25.5245663, 23.4864950, 23.5963224, 28~\r\n$ site      <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", ~\r\n\r\nStudy questions\r\nWe want to evaluate whether the mass of the mussels we collected differs between the two field sites.\r\nH0: Mussel body mass is not statistically different between the two sites.\r\nH1: Mussel body mass is statistically different between the two sites.\r\nExploratory data analysis\r\nThe first step in our statistical analysis should familiarising ourselves with the data. To do this, you should be performing exploratory data analysis (EDA).\r\n(i) Check the data corresponds to the experimental design\r\nThe first thing I usually do is check that the number of replicates per treatment is in line with the experimental design. In other words, we sampled 30 mussels per site, so let’s just check that each site has 30 mussel body mass measurements.\r\n\r\n\r\n# Start with the data frame\r\ndata %>%\r\n  # Calculate the number of replicates per 'site' \r\n  dplyr::count(site)\r\n\r\n\r\n  site  n\r\n1    A 30\r\n2    B 30\r\n\r\nAs expected, there are 30 rows of data per site. Good start!\r\n(ii) Visualise the data\r\nThe next step is to plot your data. You should be noting aspects such as:\r\nIs there a visual difference in means or the distributions of values in the different groups?\r\nIs there more variation in one group vs another group?\r\n\r\n\r\n# Start with the data frame\r\ndata %>% \r\n  ggplot(data = ., aes(x = site,\r\n                       y = body_mass)) +\r\n  geom_boxplot()\r\n\r\n\r\n\r\n\r\nIn this case, it looks like the median body mass (indicated by the bold black line) of mussels collected at site A is lower than than at site B). The variance (i.e. spread of body mass values along the y-axis) appears to be relatively similar between the two sites.\r\nFitting the model\r\nFitting a GLM is relatively simple in R. All we need to do is tell it what is our response variable (the response variable is the measurement we are interested in). Here, the response variable is (body_mass). We then specify our predictor variables to the right-hand side of this weird ~ (tilde) symbol. Our predictor variables are things we have recorded that we believe could be affecting the response variable. Here, our predictor variable was site. We need to tell R where these data are stored (data), and that we want a gaussian GLM (family = gaussian()).\r\n\r\n\r\nmod1 <- glm(body_mass ~ site, \r\n            data = data,\r\n            family = gaussian(link = \"identity\"))\r\n\r\n\r\n\r\nEvaluating model fit\r\nBefore we look at the results from our model, we must first check whether the GLM that we fit was an appropriate choice for our data. We do that by looking at model diagnostics. Model diagnostics rely heavily on calculating and visualising residuals. For example, let’s assume we had fit a model looking at whether body length is a predictor of body mass below. The residuals are the filled black circles, indicating the deviation between our observed data value and its expected value under the given model (indicating by the unfilled circle). Simply, residuals are the observed data value minus its expected value from a model. GLM’s make assumptions about the distribution of the residuals which we are going to unpack below.\r\n\r\n\r\n\r\nIn my opinion, the easiest and most informative way perform model diagnostics check in R is using the amazing package DHARMa. Below, we are going to plot two different graphs to evaluate whether our choice of model was okay.\r\nBefore we make any plots, we have to get DHARMa to produce residuals for us to use in our plots. We will feed these residuals into the different plotting functions.\r\n\r\n\r\nsimulated_resids <- DHARMa::simulateResiduals(fittedModel = mod1, \r\n                                              plot = F)\r\n\r\n\r\n\r\n(i) Plot #1: QQPlot\r\nThe QQ plot tells us whether our data conforms to the distribution we specified in the family argument in the GLM call above (remember: we said family = gaussian). If our GLM is a good fit to the data, the white triangles will fall approximately on the red 1:1 line and the KS test P-value will be greater than 0.05. The Kolmorogorov-Smirnoff test (KS test) is a formal statistical test to evaluate whether our data follow the distribution we specified in the family argument in the GLM call above.\r\n\r\n\r\nDHARMa::plotQQunif(simulated_resids)\r\n\r\n\r\n\r\n\r\nHere, we can see two important things. Firstly, the white triangles fall approximately along the red 1:1 line, indicating that our residuals (the white triangles) approximate the data distribution we specified in family = .... Secondly, the Kolmorogorov-Smirnoff test (KS test) provides further support that the residuals from our model were not significantly different from the the data distribution we specified in family = .... Taken together, the QQplot shows us that our residuals are approximately normally distributed. Great!\r\n(ii) Plot #2: Residuals vs Predicted Plot\r\nThis plots allows us to evaluate whether our data demonstrates heteroscedasticity. This is just a fancy way to say that the variance in the data is systematically dependent on some variable in the model. If you remember back to undergraduate statistics courses, linear models typically assume that the data displays equal variances across groups or numeric variables. When our predictor variables are categorical, such as in the current model (remember site was our predictor variable with two levels, siteA and siteB), this plot will show boxplots with each level (e.g. each site) getting its own boxplot.\r\nIf our GLM was a good fit, we would like the boxplots to be centered between y = 0.25 to y = 0.75, with the bold black line falling approximately on the y = 0.5 line. DHARMa has recently introduced a nice function which automatically produces a formal statistical test for within-group uniformity and between-group homogeneity of variances. Ultimately, we want both of these tests to return n.s. meaning a non-significant result.\r\n\r\n\r\nDHARMa::plotResiduals(simulated_resids)\r\n\r\n\r\n\r\n\r\nThe plot shows us that there aren’t major concerns over unequal variances. While we would like to see the left-hand boxplox grey-shaded area range from y = 0.25 to 0.75 (it ranges from y = 0.33ish to 0.80), both the test for uniformity and unequal variances test were n.s.. In later posts, we will unpack model diagnostics in much greater detail. Stay tuned.\r\nStatistical inference\r\nNow to the bit of the analysis that most ecologists are most interested in (at least to appease their reviewers: assessing statistical significance and calculating p-values). Here, we perform statistical inference, which basically means we are going to evaluate “which [model] coefficients are non-zero beyond a reasonable doubt, implying meaningful associations between covariates and the response?” (Tredennick et al. 2021).\r\nTo do this, we will use a Likelihood Ratio Test (LRT). When we only have 1 predictor variable (here: site), we can calculate p-values using type I sum-of-squares (SOS). SOS’s are just different ways that we ask R to calculate p-values.\r\nPLEASE DO NOT USE SUMMARY() - THIS WILL PRODUCE THE WRONG P-VALUES WHEN YOU HAVE MORE THAN 1 PREDICTOR VARIABLE\r\n\r\n\r\n# Perform LRT with type I sum-of-squares \r\nanova(mod1,\r\n      test = \"Chisq\")\r\n\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel: gaussian, link: identity\r\n\r\nResponse: body_mass\r\n\r\nTerms added sequentially (first to last)\r\n\r\n     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \r\nNULL                    59     8481.1              \r\nsite  1     1498        58     6983.1 0.0004198 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nSo, the LRT tells us that mussel body_mass was statistically significantly different between sites (\\(\\chi\\)2 = 1498, d.f. = 1, P < 0.001).\r\nMake a figure\r\nNow we are going to make a figure to summarise our findings and that you can include in your thesis or paper.\r\n\r\n\r\n# Calculate mean +- standard error\r\ndata %>%\r\n  ggplot(data = ., aes(x = site,\r\n                       y = body_mass,\r\n                       fill = site)) +\r\n  geom_boxplot() +\r\n  # Fill the boxes by site\r\n  scale_fill_grey(start = 0.6) +\r\n  # Add significance letters\r\n  scale_x_discrete(\"Site \",\r\n                   labels = c(\"A\", \"B\")) +\r\n  annotate(\"text\", x = 1, y = 40, label = \"a\") +\r\n  annotate(\"text\", x = 2, y = 60, label = \"b\") +\r\n  # Change axis labels\r\n  labs(y = \"Mussel body mass (mg)\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConclusions\r\nThere we go!!! You are now equipped to run your own GLM. Please let me know if there are any improvements or what topics you would like me to cover going forward.\r\n\r\n\r\n\r\nTredennick, Andrew T, Giles Hooker, Stephen P Ellner, and Peter B Adler. 2021. “A Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology.” Ecology 102 (6): e03336.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-04-gaussian-general-linear-model-glm/gaussian-general-linear-model-glm_files/figure-html5/glm_gaussian_single_categorical_plot-1.png",
    "last_modified": "2021-08-15T21:22:28+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to Stats for Scared Ecologists",
    "description": "Why this blog?",
    "author": [
      {
        "name": "Guy F. Sutton",
        "url": "https://twitter.com/Guy_F_Sutton"
      }
    ],
    "date": "2021-07-30",
    "categories": [],
    "contents": "\r\nJust mentioning statistics, ANOVA, or the dreaded R is enough to stir up fear and panic for many ecologists. Most of us chose ecology so that we could leave the dreadful memories of high-school calculus and algebra behind us. However, when you go to pick your undergraduate courses, the university requires all ecology majors to have some mathematics or statistics credits, nevermind your MSc or PhD advisor telling you to go perform X or Y analysis on your hard-earned data… This latter situation usually results in the student collapsing into a heap in the corner of their lab.\r\nStatistical analyses are a multi-dimensional problem solving task, which in my opinion, can be divided into two relatively simple tasks for the common ecologist. Firstly, you obviously need to understand the statistics. This involves aspects such as picking an appropriate statistical analysis for the data you have and the questions you wish to ask, understanding what the analyses does and how to interpret the output from your analysis (read: how to explain your statistical analysis in ecological terms!). Secondly, most contemporary statistical analyses in ecology require the use of appropriate software to run your analyses. An underappreciated and infrequently discussed aspect of statistical analyses is the skill of communicating with your own computer. Anyone who has sat for days trying to figure out why their R code won’t run properly, or even why they can’t install Python on their computers, may stop reading at this point!\r\nThis blog is here to help the scared ecologist learn the statistical methods required for their analyses and how to run these analyses on their own computers. I will primarily be using R, as this appears to be the gold-standard for statistical analyses in ecology, but I will try to introduce other software as we go (e.g. Julia).\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-04T21:45:20+02:00",
    "input_file": {}
  }
]
